---
title:  PaperNote-Deep Image Prior
date: 2017-12-15 19:49:00
categories:
- PaperNote
tags:
- PaperNote
---


%!TEX program = xelatex
%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

% 添加中文自动换行
\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt

\documentclass[10pt, letterpaper]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{fontspec,xunicode,xltxtra,graphicx}
\setmainfont{Hiragino Sans GB}
% 首行缩进2个字符,
\usepackage{indentfirst}
\setlength{\parindent}{2em}

\title{Deep Image Prior Note\\[1ex]\begin{large}阅读Deep Image Prior及其Supplementary Material\end{large}}
\author{钮孟洋}
\date{December 12, 2017}

\begin{document}
\maketitle{}

最近出现了一篇非常特别的文章，名叫“Deep Image Prior”，彻底脱离了SRCNN、SRGAN的框架，提出了一种非常新颖的图像重建思想：无需使用预训练模型，无需使用大量数据集训练网络，在模型学习之前，神经网络就能够有效地掌握大量low-level的图像先验统计学信息。作者用实验证明，随机初始化的模型就能够成为一个“人工先验”，能够直接在去噪、超分辨率、修补等图像重建应用中有很好的性能。

文章还突出了标准生成器网络架构所捕获的归纳偏差（inductive bias）。它还弥补了两种通用的图像恢复方法之间的鸿沟：即使用深度卷积网络的基于学习的方法和基于手动图像先验（如自相似性）的无学习方法。

因为这种方法思想和结构非常简单，所以并没有限定的适用范围，应用非常广泛。

\section{Method}

1、用随机参数初始化深度卷积网络f；\par
2、令f的输入为固定的随机编码z；\par
3、令f的目标为：输入z，输出x。以此训练f的参数；\par
4、注意选择合适的损失函数。例如对于降噪问题可关注整体的MSE，对于填充问题就应该只关心不需要填充的位置的MSE；\par
5、当训练很久之后，f可实现输出一模一样的x；\par
6、但如果在训练到一半时打断f，会发现它会输出一幅“修复过的x”。\par
~\par
此方法可以被视作是解决如下问题：

$$
x^*=\min_{x}E(x;x_0)+R(x)
$$

其中$x_0$是被破坏了的低质量图像，$R(x)$是正则项。在文章中，作者使用神经网络捕捉到的绝对先验替换$R(x)$，即

$$
\theta^*=\arg\min_{\theta}E(f_\theta(z);x_0)\qquad x^*=f_{\theta^*}(z)
$$

其中$\theta$为网络参数，$z$为随机噪声。固定噪声$z$，优化$\theta$，当达到最优点时，$x^*=f_{\theta^*}(z)$。

以下图为例，对一张噪声污染的图进行多次迭代，在不同的迭代次数上网络输出如下图所示：

\begin{center}
\includegraphics[width=350pt]{Figure3.png}
\end{center}\par

可以看到，在2400次迭代时网络输出高质量图，经过50000次迭代，网络输出接近带噪声原图。

那么，要如何确定何时能停止迭代呢？作者做了一个测试，作者用MSE作为loss，如下所示：

$x_0)={\|x-x_0\|}^2
$$$
E(x;

即是如下的优化问题：

$$
\min_{\theta}{\|f_\theta(z)-x_0\|}^2
$$

实验结果如图所示，

\begin{center}
\includegraphics[width=180pt]{Figure2.png}
\end{center}\par

即网络能够很快收敛到高质量图片，而难以收敛到噪声图片。

\section{Summary}

Deep Image Prior的重要特点是，网络由始至终，仅使用了输入的被破坏过的图像做为训练。它没有看过任何其它图像，也没有看过正常的图像。

这就是说，深度卷积网络先天就拥有一种能力：它会先学会x中“未被破坏的，符合自然规律的部分”，然后才会学会x中“被破坏的部分”。例如，它会先学会如何复制出一张没有噪点的x，然后才会学会复制出一张有噪点的x。

换而言之，深度卷积网络先天就了解自然的图像应该是怎样的。所以我们也可以理解为什么GAN的方法这么简单却效果这么好。这无疑来自于卷积的不变性，和逐层抽象的结构。
\end{document}
